cal.start <- start
cal.start.n <- start.n
val.min <- c(rep(1, ncol(prco)))
val.max <- c(rep(1, ncol(prco)))
for (i in 1:ncol(prco))
{
blub <- which(prco[-cal.start.n, i] == min(prco[-cal.start.n,
i]))
val.min[i] <- blub[sample(length(blub), 1)]
bla <- which(prco[-cal.start.n, i] == max(prco[-cal.start.n,
i]))
val.max[i] <- bla[sample(length(bla), 1)]
}
val.min <- rownames(prco[-cal.start.n, ])[val.min]
val.max <- rownames(prco[-cal.start.n, ])[val.max]
val.start <- unique(c(val.min, val.max))
val.start.n <- match(val.start, rownames(inp))
cal.val <- c(cal.start, val.start)
cal.val.start <- match(c(cal.start, val.start), rownames(inp))
euc <- as.data.frame(as.matrix(dist(prco)))
inp.start <- rownames(prco)[-cal.val.start]
inp.start.b <- inp.start
val <- val.start
stop<-n#min(c(n,length(val.start)))
k<-1
for (k in 1:(stop))
{
test <- apply(euc[inp.start.b, val], 1, min)
bla <- names(which(test == max(test)))
val <- c(val, bla)
inp.start.b <- inp.start.b[-(which(match(inp.start.b,
bla) != "NA"))]
}
val.n <- match(val, rownames(inp))
cal.n <- c(1:nrow(inp))[-val.n]
cal <- rownames(inp)[cal.n]
#tmp fix for problem in function
n<-ceiling(per.n*nrow(inp))
if(n<1){n<-1}
tst.id<-unique(val.n)
if(n>length(tst.id)){n=length(tst.id)}
val.n<-sample(tst.id,n)
cal.n<-c(cal.n,tst.id[!tst.id%in%val.n])
cal <- rownames(inp)[cal.n]
val<-rownames(inp)[val.n]
output <- list(`Calibration and validation set` = va,
`Number important PC` = pc, `PC space important PC` = prco,
`Chosen sample names` = "NULL", `Chosen row number` = "NULL",
`Chosen calibration sample names` = unique(cal), `Chosen calibration row number` = unique(cal.n),
`Chosen validation sample names` = unique(val), `Chosen validation row number` = unique(val.n))
}
class(output) <- "ken.sto"
return(output)
}
#wrapper to iterate ken.sto2
duplex.select<-function(data,ken.sto2.obj,percent.in.test){
#determine how many more are needed
start.have<-ken.sto2.obj$`Chosen validation sample names`
need<-percent.in.test*nrow(data)-length(start.have)
#don't do anything if there are enough
if(need>0)
{
#extract from remainning data
have<-start.have
while(need>0)
{
tmp.data<-data[!rownames(data)%in%have,]
more<-ken.sto2(tmp.data, per = "TRUE", per.n = percent.in.test, num = 7, va = "TRUE")
now.have<-more$`Chosen validation sample names`
need<-percent.in.test*nrow(data)-(length(now.have)+length(have))
have<-c(have,now.have)
}
#adjust for too many selected
drop<-NA
if(need<0)
{
drop<-now.have[sample(length(now.have),abs(need))]
}
new.obj<-have[!have%in%drop]
#objects to return
`Chosen validation sample names`=c(new.obj)
`Chosen validation row number`= c(1:nrow(data))[rownames(data)%in%new.obj]
`Chosen calibration sample names`= rownames(data)[!rownames(data)%in%`Chosen validation sample names`]
`Chosen calibration row number` =c(1:nrow(data))[rownames(data)%in%`Chosen calibration sample names`]
}else{
`Chosen validation sample names`=ken.sto2.obj$`Chosen validation sample names`
`Chosen validation row number`= ken.sto2.obj$`Chosen validation row number`
`Chosen calibration sample names`= ken.sto2.obj$`Chosen calibration sample names`
`Chosen calibration row number` =ken.sto2.obj$`Chosen calibration row number`
}
output<-list(`Chosen validation row number`= `Chosen validation row number`,
`Chosen validation sample names`=`Chosen validation sample names`,
`Chosen calibration sample names` = `Chosen calibration sample names`,
`Chosen calibration row number` = `Chosen calibration row number`)
}
#function to calculate included/excluded feature model stats
optimize.OPLS.feature.select<-function(model,feature.subset,permute=TRUE,train.test.index,progress=TRUE,test="perm.test",...){
#need to know OPLS model args (*later store in model and use this a reference)
#not sure why this is missing
ntests<-ncol(train.test.index)
#selected model stats (TODO: avoid recalculating the model!)
data<-model$data[[1]][,feature.subset,drop=F]
model1<-make.OSC.PLS.model(pls.y=model$y[[1]],pls.data=data,comp=model$total.LVs[1],OSC.comp=max(model$OSC.LVs), validation = model$model.description$validation,method=model$model.description$method, cv.scale=model$model.description$cv.scale,return.obj="stats",progress = progress,...)
if(permute==TRUE){
#permutation
sel.permuted.stats <-permute.OSC.PLS.train.test(data,pls.y = model$y[[1]],perm.n = ntests,train.test.index = train.test.index,comp =  model$total.LVs[1],OSC.comp = max(model$OSC.LVs),progress = progress,...)
# sel.permuted.stats <- permute.OSC.PLS(data = data, y = model$y[[1]], n = ntests, ncomp = model$total.LVs[1], osc.comp=max(model$OSC.LVs), progress = progress, train.test.index = train.test.index,...) #...
} else {
sel.permuted.stats<-NULL
}
#training/testing to get robust model stats
sel.OPLS.train.stats <- OSC.PLS.train.test(pls.data = data, pls.y = model$y[[1]], train.test.index, comp = model$total.LVs[1], OSC.comp = max(model$OSC.LVs), cv.scale = model$model.description$cv.scale, progress = progress,...) # ...
sel.OPLS.model<-OSC.validate.model(model = model1, perm = sel.permuted.stats, train = sel.OPLS.train.stats,test,...)
#excluded model stats
data<-model$data[[1]][,!feature.subset]
model2<-make.OSC.PLS.model(pls.y=model$y[[1]],pls.data=data,comp=model$total.LVs[1],OSC.comp=max(model$OSC.LVs), validation = model$model.description$validation,method=model$model.description$method, cv.scale=model$model.description$cv.scale,return.obj="stats",progress = progress,...)
if(permute==TRUE){
#permutation
ex.permuted.stats <-permute.OSC.PLS.train.test(data,pls.y = model$y[[1]],perm.n = ntests,train.test.index = train.test.index,comp =  model$total.LVs[1],OSC.comp = max(model$OSC.LVs),progress = progress,...)
# ex.permuted.stats <- permute.OSC.PLS(data = data, y = model$y[[1]], n = ntests, ncomp = model$total.LVs[1], osc.comp=max(model$OSC.LVs), progress = progress, train.test.index = train.test.index,...) #...
} else {
ex.permuted.stats<-NULL
}
#training/testing to get robust model stats
ex.OPLS.train.stats <- OSC.PLS.train.test(pls.data = data, pls.y = model$y[[1]], train.test.index, comp = model$total.LVs[1], OSC.comp = max(model$OSC.LVs), cv.scale = model$model.description$cv.scale, progress = progress,...) # ...
ex.OPLS.model<-OSC.validate.model(model = model2, perm = ex.permuted.stats, train = ex.OPLS.train.stats,test,...)
full.sel.model.comparison<-OSC.PLS.model.compare(model1=sel.OPLS.train.stats, model2=ex.OPLS.train.stats,test,...)
#create final table
out<-data.frame(cbind(model=c(rep("selected",3),rep("excluded",3),"comparison"),rbind(as.matrix(sel.OPLS.model),as.matrix(ex.OPLS.model),as.matrix(full.sel.model.comparison)[3,,drop=F])))
list(selected.train=sel.OPLS.train.stats,selected.permuted=sel.permuted.stats,excluded.train=ex.OPLS.train.stats,excluded.permuted=ex.permuted.stats,summary=out)
}
#get classification performance statistics
O.PLS.DA.stats<-function(truth,pred){
#
check.get.packages("ROCR")
# library(ROCR)
# # library(caret) #need e1071
# library(hmeasure)
y.range<-range(as.numeric(truth))
mid<-mean(y.range)
binned.pred<-pred
binned.pred[binned.pred<mid]<-y.range[1]
binned.pred[binned.pred>=mid]<-y.range[2]
# scaled.pred<-rescale(as.numeric(pred),y.range)
# scaled.pred[scaled.pred<mid]<-y.range[1]
# scaled.pred[scaled.pred>=mid]<-y.range[2] # not sure what to do with a prediction == the mid point
#get AUC
mod.AUC<-function(pred,truth){
# pred1 <- prediction(pred, truth)
#perf <- performance(pred1, measure="tpr", x.measure="fpr")
# plot(perf,lty=1,lwd=4,col="#9400D350") # plot not interesting with so  few measurements
# add precision recall http://stackoverflow.com/questions/8499361/easy-way-of-counting-precision-recall-and-f1-score-in-r
unlist(performance(prediction(pred, truth),measure= "auc")@y.values)
}
#misclassCounts(binned.pred,truth)  # library(hmeasure)
AUC<-tryCatch(mod.AUC(pred=binned.pred,truth=truth),error=function(e){NA}) # protect errors due to !=2 groups
# AUC<-mod.AUC(pred=binned.pred,truth=truth) # get NA when groups !=2
# get other metrics
# library(hmeasure) # using modified fxn which accepts inputs other than 1 and 0
results<-tryCatch(misclassCounts2(pred=binned.pred,truth=truth)$metrics ,error=function(e){NULL}) # protect errors due to >2 groups or use caret::confusionMatrix
#happens when there are perfect predictions
# library(caret)
# results<-tryCatch(confusionMatrix(binned.pred,as.numeric(truth)),error=function(e){"error"}) # protect errors due to >2 groups
if(is.null(results)){results<-list();results$byClass[1:2]<-NA}
# res<-data.frame(AUC=AUC,sensitivity=results$byClass[1], specificity=results$byClass[2])
res<-data.frame(AUC=AUC,results)
rownames(res)<-"model"
return(res)
}
#modified hmeasure::misclassCounts to accept class values besides 0 and 1
# limited to only 2 classes
misclassCounts2<-function (pred,truth){
vals<-sort(unique(truth),decreasing=TRUE) # smaller value is not the class
TP <- sum(pred == vals[1] & truth == vals[1])
FP <- sum(pred == vals[1] & truth == vals[2])
TN <- sum(pred == vals[2] & truth == vals[2])
FN <- sum(pred == vals[2] & truth == vals[1])
conf.matrix <- data.frame(pred.1 = c(TP, FP), pred.0 = c(FN,
TN))
row.names(conf.matrix) <- c("actual.1", "actual.0")
ER <- (FP + FN)/(TP + FP + TN + FN)
Sens <- TP/(TP + FN)
Spec <- TN/(TN + FP)
Precision <- TP/(TP + FP)
Recall <- Sens
TPR <- Recall
FPR <- 1 - Spec
F <- 2/(1/Precision + 1/Sens)
Youden <- Sens + Spec - 1
metrics <- data.frame(ER = ER, Sens = Sens, Spec = Spec,
Precision = Precision, Recall = Recall, TPR = TPR, FPR = FPR,
F = F, Youden = Youden)
return(list(conf.matrix = conf.matrix, metrics = metrics))
}
# generic mean squared error of prediction
.MSEP<-function(actual,pred){
if(is.null(dim(actual))){
mean((actual-pred)^2)
} else {
sapply(1:ncol(actual),function(i){mean((actual[,i]-pred[,i])^2)})
}
}
#wrapper to carry out multiple feature selection and model validation runs
multi.OPLS.feature.select<-function(model,filter,ocomp=max(model$OSC.LVs),extra=NULL,train.test.index=NULL,progress=TRUE,...){
library(plyr)
.model<-get.OSC.model(obj=model,OSC.comp=ocomp)
optimal.feature.selection<-lapply(1:length(filter),function(i){
top<-filter[i]
#this step can be done only once
opts<-PLS.feature.select(model$data[[1]],pls.scores=.model$scores[,][,1,drop=F],pls.loadings=.model$loadings[,][,1,drop=F],pls.weight=.model$loadings[,][,1,drop=F],plot=FALSE,p.value=1,FDR=FALSE,cut.type="number",top=top,separate=FALSE)
optim<-optimize.OPLS.feature.select(model=model,feature.subset=opts$combined.selection,permute=TRUE,train.test.index=train.test.index,progress=progress,...) # check variance explained in X
if(progress==TRUE){print(paste0(round(i/length(filter)*100,0)," % complete"))}
# return model and permuted results
rbind(cbind(type="model",rbind(data.frame(vars=top,model="included",optim$selected.train$performance),data.frame(vars=top,model="excluded",optim$excluded.train$performance))),
cbind(type="permuted",rbind(data.frame(vars=top,model="included",optim$selected.permuted$performance),data.frame(vars=top,model="excluded",optim$selected.permuted$performance))))
})
#summary
obj<-do.call("rbind",optimal.feature.selection)
#get summary
library(plyr)
means<-ddply(obj,.(type,vars,model),colwise(mean,na.rm=TRUE))
sds<-ddply(obj,.(type,vars,model),colwise(sd,na.rm=TRUE))
#calculate p-values for the comparison
#fxn
compare.mods<-function(mod.vals,perm.vals,test="perm.test",...){
lapply(1:ncol(mod.vals),function(i){
if(names(mod.vals[i])%in%c("RMSEP","ER","FPR")) {dir<-">"} else {dir<-"<"} # used for permutation tests
switch(test,
"t.test"        = group.test(mod=mod.vals[,i],perm=perm.vals[,i]),
"perm.test" = perm.test(mod=mod.vals[i],perm=perm.vals[,i],dir),
"perm.test2" = perm.test(mod=mod.vals[i],perm=perm.vals[,i],dir,type=2))
})
}
#hack objects to fit
#included to excluded
tmp.l<-split(obj,obj$type)
#included vs. excluded
p.vals<-ddply(tmp.l[["model"]],.(vars),function(tmp,...){
tmp.obj<-tmp[,-c(1:3)]
id<-tmp$model=="included"
res<-data.frame(compare.mods(tmp.obj[id,],tmp.obj[!id,],...))
colnames(res)<-colnames(tmp)[-c(1:3)]
return(res)
})
#model vs. permuted
tmp.l<-split(obj,obj$model)
#included vs. permuted
p.vals2<-ddply(tmp.l[["included"]],.(vars),function(tmp,...){
tmp.obj<-tmp[,-c(1:3)]
id<-tmp$type=="model"
res<-data.frame(compare.mods(tmp.obj[id,],tmp.obj[!id,],...))
colnames(res)<-colnames(tmp)[-c(1:3)]
return(res)
})
#excluded vs. permuted
#included vs. permuted
p.vals3<-ddply(tmp.l[["excluded"]],.(vars),function(tmp,...){
tmp.obj<-tmp[,-c(1:3)]
id<-tmp$type=="model"
res<-data.frame(compare.mods(tmp.obj[id,],tmp.obj[!id,],...))
colnames(res)<-colnames(tmp)[-c(1:3)]
return(res)
})
#p.values
p.values<-data.frame(rbind(cbind(type="included vs. excluded",p.vals),cbind(type="included vs. permuted",p.vals2),cbind(type="excluded vs. permuted",p.vals3)))
list(all.results=obj,summary=list(mean=means,sd=sds,p.values=p.values))
}
#plot the results of multi.OPLS.feature.select
plot.multi.OPLS.feature.select<-function(object,extra=NULL,objects=c("RMSEP","Q2","AUC","F","Sens","Spec","Precision","Recall")){
check.get.packages("gridExtra")
means<-object$summary$mean[,-c(1:3)]
sds<-object$summary$sd[,-c(1:3)]
p.vals<-object$summary$p.values[,-c(1:2)]
model<-object$summary$mean$model
vars<-object$summary$mean$vars
model.type<-join.columns(data.frame(model,object$summary$mean$type),"_")
#filter error columns
f<-function(x){sum(is.na(x))}
id<-!apply(means,2,f)==nrow(means)&colnames(means)%in%objects #exclude all NA columns
means<-means[,id,drop=FALSE]
sds<-sds[,id,drop=FALSE]
p.vals<-p.vals[,id]
.theme<- theme(
axis.line = element_line(colour = 'gray', size = .75),
panel.background = element_blank(),
plot.background = element_blank()
)
plot.list<-list()
k<-1
for(i in 1:ncol(means)){
value<-colnames(means)[i]
tmp<-data.frame(value=means[,value],error=sds[,value],model=model.type,vars=vars)
vlines<-p.vals$var[p.vals[,value]<=0.05]
if(length(vlines)>0){show.sig<-geom_vline(xintercept=vlines,linetype="dotted")}	else {show.sig<-NULL}
plot.list[[k]]<-ggplot(data = tmp, aes(x = vars, y = value,group=model) ) +
geom_errorbar(aes(ymin = value - error, ymax = value + error, fill=model,color=model), size=1, width=.1) + # add error bars (do so before geom_point so the points are on top of the error bars)
geom_line(aes(color=model),size=1) + # join points with lines (specify this before geom_point, or the lines will be drawn over the shapes)
geom_point(aes(shape=model, fill=model,color=model), size=5) +ylab(value) +
xlab("cutoff") + .theme + scale_x_continuous("cutoff", breaks=unique(tmp$vars)) +show.sig +extra
k<-k+1
}
#plot
if(length(plot.list)>1){
do.call("grid.arrange",plot.list)
} else {print(plot.list[[1]])}
}
#extract best model
best.OPLS.features<-function(obj=res,decreasing = c("RMSEP"),measures=c("AUC","Sens","Spec","Precision","Recall","TPR","F","Youden")){
#rank
tmp<-obj$summary$mean[obj$summary$mean$model=="included"&obj$summary$mean$type=="model",,drop=FALSE]
.tmp<-tmp[,colnames(tmp)%in%measures,drop=FALSE]
# variables which need small is best rank need to be inverted
.tmp[,colnames(.tmp)%in%decreasing]<-1/.tmp[,colnames(.tmp)%in%decreasing]
.rank<-apply(.tmp,2,rank) #rank is decreasing
rmat<-matrix(.rank,,length(measures))
rowid<-matrix(1:nrow(rmat),nrow(rmat),ncol(rmat))[which.max(rmat)]
tmp[rowid,,drop=FALSE]
}
#iteratively split a vector into fractional units taking the floor
split.vector<-function(n,step=.5){
#adapted from randomForest::rfcv
k <- floor(log(n, base = 1/step))
n.var <- round(n * step^(0:(k - 1)))
same <- diff(n.var) == 0
if (any(same))
n.var <- n.var[-which(same)]
if (!1 %in% n.var)
n.var <- c(n.var, 1)
return(n.var)
}
#various tests
test<-function(){
data(mtcars)
#O-PLS Regression
{
pls.data<-mtcars[,-1]
pls.y<-mtcars[,1,drop=F]
opls.results<-make.OSC.PLS.model(pls.y,pls.data,
comp=2,
OSC.comp=1,
validation = "LOO",
cv.scale = TRUE,
train.test.index=NULL,
progress=TRUE)
final.opls.results<-get.OSC.model(obj=opls.results,OSC.comp=1)
(opls.model.text<-data.frame("Xvar"=c(0,round(cumsum(final.opls.results$Xvar)*100,2)),"Q2"=final.opls.results$Q2,"RMSEP"= final.opls.results$RMSEP)	)
#predict values using training test split
#test
ntests<-1
strata<-NULL # use to control equivalent sampling from groups
#train/test index
train.test.index <- test.train.split(nrow(pls.data), n = ntests, strata = strata, split.type = "random", data = pls.data)
mods<-make.OSC.PLS.model(pls.y,pls.data,
comp=2,
OSC.comp=1,
validation = "LOO",
cv.scale = TRUE,
train.test.index=train.test.index,
progress=FALSE)
#view predictions for test data
final.opls.results2<-get.OSC.model(obj=mods,OSC.comp=1)
fitted<-final.opls.results2$predicted.Y
(RMSEP<-(.MSEP(actual=pls.y[train.test.index=="test",],pred=fitted))^.5)
# carry out multiple runs of trainning/testing
ntests<-10
#train/test index
train.test.index <- test.train.split(nrow(pls.data), n = ntests, strata = strata, split.type = "random", data = pls.data)
multi.train.test<-OSC.PLS.train.test(pls.data = pls.data, pls.y = pls.y, train.test.index, comp = mods$total.LVs[1], OSC.comp = max(mods$OSC.LVs), cv.scale = mods$model.description$cv.scale, progress = TRUE) # ...
# carry out model permutation testing
# multi.permute<-permute.OSC.PLS(data = pls.data, y = pls.y, n = ntests, ncomp = mods$total.LVs[1], osc.comp=max(mods$OSC.LVs), progress = TRUE, train.test.index = train.test.index) #...
multi.permute<-permute.OSC.PLS.train.test(pls.data = pls.data, pls.y = pls.y, perm.n = ntests, comp = mods$total.LVs[1], OSC.comp=max(mods$OSC.LVs), progress = TRUE, train.test.index = train.test.index)
#compare actual to permuted model performance
(model.validation<-OSC.validate.model(model = mods, perm = multi.permute, train = multi.train.test,test="perm.test"))
#feature selection
opts<-PLS.feature.select(pls.data,pls.scores=final.opls.results$scores[,][,1,drop=F],pls.loadings=final.opls.results$loadings[,][,1,drop=F],pls.weight=final.opls.results$loadings[,][,1,drop=F],plot=FALSE,p.value=0.1,FDR=TRUE,cut.type="number",top=3,separate=FALSE)
# make s-plot plus
plot.S.plot(obj=opts,return="all")
#calculate included and excluded feature statistics
(optim<-optimize.OPLS.feature.select(model=opls.results,feature.subset=opts$combined.selection,permute=TRUE,train.test.index,progress=FALSE,test="perm.test") )# check variance explained in X
#optimize model feature selections
filter<-seq(3,ncol(pls.data)-3) # number of variables to keep
res<-multi.OPLS.feature.select(model=opls.results,filter=filter,plot=FALSE,OPLSDA=TRUE,train.test.index=train.test.index, test="perm.test") # use full model without training split as input
plot.multi.OPLS.feature.select(res,objects=c("RMSEP","Q2")) # view results
best.OPLS.features(res) # extract best model
}
#O-PLS-DA
#--------------------------
{
pls.data<-mtcars[,!colnames(mtcars)%in%"am"]
pls.y<-mtcars$am
opls.results<-make.OSC.PLS.model(pls.y,pls.data,
comp=2,
OSC.comp=1,
validation = "LOO",
cv.scale = TRUE,
train.test.index=NULL,
progress=TRUE,
OPLSDA=TRUE)
final.opls.results<-get.OSC.model(obj=opls.results,OSC.comp=1)
(opls.model.text<-data.frame("Xvar"=c(0,round(cumsum(final.opls.results$Xvar)*100,2)),"Q2"=final.opls.results$Q2,"RMSEP"= final.opls.results$RMSEP)	)
#classification performance (on the training data)
opls.results$OPLSDA
# predict class labels
ntests<-1
strata<-pls.y # use to control equivalent sampling from groups
#train/test index
train.test.index <- test.train.split(nrow(pls.data), n = ntests, strata = strata, split.type = "random", data = pls.data)
opls.results2<-make.OSC.PLS.model(pls.y,pls.data,
comp=3,
OSC.comp=1,
validation = "LOO",
cv.scale = TRUE,
train.test.index=train.test.index,
progress=FALSE,
OPLSDA=TRUE)
#get classification stats (on test data)
final.opls.results<-get.OSC.model(obj=opls.results2,OSC.comp=1)
final.opls.results$OPLSDA.stats # perfect model
#carry out model feature selection and validation
ntests<-5
strata<-pls.y # use to control equivalent sampling from groups
#train/test index
train.test.index <- test.train.split(nrow(pls.data), n = ntests, strata = strata, split.type = "random", data = pls.data)
filter<-seq(3,ncol(pls.data)-3)
#try different thresholds for feature selection
#wrapper to fit multiple models and validate
res<-multi.OPLS.feature.select(model=opls.results,filter=filter,plot=FALSE,OPLSDA=TRUE,train.test.index=train.test.index) # use full model without training split as input
plot.multi.OPLS.feature.select(res,objects="RMSEP") # view results
best.OPLS.features(res) # extract best model
#extract best model
#
O.PLS.DA.stats(pred=round(runif(10),0),truth=round(runif(10),0))
}
}
pls.data<-mtcars[,-1]
pls.data
mtcars[,1]
mtcars
pls.data<-mtcars[,-1]
pls.y<-mtcars[,1,drop=F]
opls.results<-make.OSC.PLS.model(pls.y,pls.data,
comp=2,
OSC.comp=1,
validation = "LOO",
cv.scale = TRUE,
train.test.index=NULL,
progress=TRUE)
make.OSC.PLS.model
opls.results<-make.OSC.PLS.model(pls.y,pls.data,
comp=2,
OSC.comp=1,
validation = "LOO",
cv.scale = TRUE,
train.test.index=NULL,
progress=TRUE)
pls.data<-mtcars[,!colnames(mtcars)%in%"am"]
pls.y<-mtcars$am
opls.results<-make.OSC.PLS.model(pls.y,pls.data,
comp=2,
OSC.comp=1,
validation = "LOO",
cv.scale = TRUE,
train.test.index=NULL,
progress=TRUE,
OPLSDA=TRUE)
pls.data<-mtcars[,!colnames(mtcars)%in%"am"]
pls.y<-mtcars$am
opls.results<-make.OSC.PLS.model(pls.y,pls.data,
comp=2,
OSC.comp=1,
validation = "LOO",
cv.scale = TRUE,
train.test.index=NULL,
progress=TRUE,
OPLSDA=TRUE)
cor.mat<-devium.calculate.correlations(cbind(pls.scores,1:length(pls.scores)),results="matrix",...)
cor.mat<-devium.calculate.correlations(cbind(pls.scores,1:length(pls.scores)),results="matrix")
cor.mat<-devium.calculate.correlations(cbind(pls.scores,data.frame(1:length(pls.scores),2:(length(pls.scores)+1))),results="matrix")
corrs<-cor.mat$cor[-1,1]
ccorrs
corrs
corrs
cor.mat
cor.mat
pls.data
pls.scores
dim(pls.data)
pls.scores
devium.calculate.correlations
?rcorr
x <- c(-2, -1, 0, 1, 2)
y <- c(4,   1, 0, 1, 4)
z <- c(1,   2, 3, 4, NA)
v <- c(1,   2, 3, 4, 5)
rcorr(cbind(x,y,z,v))
corr(x,y)
cor(x,y)
cor(x,z)
?cor
cor(x,y = z)
shiny::runApp('D:/WORK/WCMC/tools/DeviumWeb-FiehnlabFix-master')
cor.mat
cor.mat$cor
cor.mat$cor[-1,1]
pls.scores
pls.data
devium.calculate.correlations(cbind(pls.scores,pls.data),results="matrix")
pls.data
pls.scores
length(pls.scores)
dim(pls.data)
devium.calculate.correlations(cbind(pls.scores,pls.data),results="matrix")
